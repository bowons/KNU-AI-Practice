{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOOfTKa8XAB9UaTK8YYJkqZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bowons/KNU-AI-Practice/blob/main/4_AI_practice.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Perceptron"
      ],
      "metadata": {
        "id": "Yf7BpfO6M1oL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K-er_3a5M0_1",
        "outputId": "6256bddd-676e-49f7-9f0d-d77bd4bc6924"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "np.float64(0.5)"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "import numpy as np\n",
        "x2 = np.array([1,0,1])\n",
        "w = np.array([-0.5, 1.0, 1.0])\n",
        "s=sum(x2*w)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "x = np.array([[1,0,0],[1,0,1],[1,1,0],[1,1,1]])\n",
        "w = np.array([-0.5, 1.0, 1.0])\n",
        "s = np.sum(x*w, axis=1)\n",
        "\n",
        "s"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OwgBnYI1SzkD",
        "outputId": "8cea03bc-cc38-4332-ba72-11933d3e6d32"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([-0.5,  0.5,  0.5,  1.5])"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## OR 데이터 인식"
      ],
      "metadata": {
        "id": "ppeSuPskD3jf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import Perceptron\n",
        "\n",
        "#훈련 집합 구축\n",
        "x = [[0,0],[0,1],[1,0],[1,1]]\n",
        "y = [-1,1,1,1]\n",
        "\n",
        "#fit 함수로 Perceptron 학습\n",
        "p = Perceptron()\n",
        "p.fit(x,y)\n",
        "\n",
        "print(\"학습된 퍼셉트론의 매개변수:\", p.coef_,p.intercept_)\n",
        "print(\"훈련집합에 대한 예측: \",p.predict(x))\n",
        "print(\"정확률 측정: \",p.score(x,y)*100,\"%\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_9FbOX7RESuQ",
        "outputId": "6ca85a3d-140d-40c0-df23-e08e7ab62035"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "학습된 퍼셉트론의 매개변수: [[2. 2.]] [-1.]\n",
            "훈련집합에 대한 예측:  [-1  1  1  1]\n",
            "정확률 측정:  100.0 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import datasets\n",
        "from sklearn.linear_model import Perceptron\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "\n",
        "#데이터 셋을 읽고 훈련 집합과 테스트 집합으로 분할\n",
        "digit=datasets.load_digits()\n",
        "x_train,x_test,y_train,y_test=train_test_split(digit.data,digit.target,train_size=0.6)\n",
        "\n",
        "# fit 함수로 Perceptron 학습\n",
        "p=Perceptron(max_iter=100, eta0=0.001, verbose=0)\n",
        "p.fit(x_train,y_train) # digit 데이터로 모델링\n",
        "\n",
        "res=p.predict(x_test) #테스트 집합으로 예측\n",
        "\n",
        "# 혼동 행렬\n",
        "conf=np.zeros((10,10))\n",
        "for i in range(len(res)):\n",
        "  conf[res[i]][y_test[i]] += 1\n",
        "print(conf)\n",
        "\n",
        "# 정확률 계산\n",
        "no_correct = 0\n",
        "for i in range(10):\n",
        "  no_correct += conf[i][i]\n",
        "accuracy = no_correct/len(res)\n",
        "print(\"테스트 집합에 대한 정확률은 \", accuracy*100, \"%입니다.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RiXU1pdoZv3u",
        "outputId": "9118c4a9-4830-4976-ed40-8ef49fc1615f"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[70.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
            " [ 1. 64.  3.  2.  4.  2.  1.  0.  6.  3.]\n",
            " [ 0.  0. 67.  0.  0.  0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0. 68.  0.  0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0. 63.  0.  0.  2.  0.  0.]\n",
            " [ 2.  0.  0.  4.  0. 69.  0.  0.  0.  2.]\n",
            " [ 0.  0.  0.  1.  1.  1. 73.  0.  1.  0.]\n",
            " [ 0.  0.  0.  3.  0.  0.  0. 66.  0.  0.]\n",
            " [ 0.  0.  1.  5.  1.  0.  0.  1. 63.  5.]\n",
            " [ 0.  0.  0.  0.  0.  0.  0.  1.  0. 63.]]\n",
            "테스트 집합에 대한 정확률은  92.62865090403338 %입니다.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "정확도의 결과가 SVM 모델보다 열등함\n",
        "- 퍼셉트론은 선형 분류기이기 때문이다."
      ],
      "metadata": {
        "id": "syyvIkmva06T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 다층 퍼셉트론 프로그래밍"
      ],
      "metadata": {
        "id": "tXbL3EJdC9nT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import datasets\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "\n",
        "# 데이터셋을 읽고 훈련 집합과 테스트 집합으로 분할\n",
        "digit=datasets.load_digits()\n",
        "x_train,x_test,y_train,y_test=train_test_split(digit.data,digit.target,train_size=0.6)\n",
        "\n",
        "#MLP 분류기 모델을 학습\n",
        "mlp=MLPClassifier(hidden_layer_sizes=(100),learning_rate_init=0.001,batch_size=32,max_iter=300,solver='sgd',verbose=True)\n",
        "mlp.fit(x_train,y_train)\n",
        "\n",
        "res=mlp.predict(x_test) #테스트 집합으로 예측\n",
        "\n",
        "#혼동 행렬\n",
        "conf=np.zeros((10,10))\n",
        "for i in range(len(res)):\n",
        "  conf[res[i]][y_test[i]] += 1\n",
        "print(conf)\n",
        "\n",
        "#정확률 계산\n",
        "no_correct = 0\n",
        "for i in range(10):\n",
        "  no_correct+=conf[i][i]\n",
        "accuracy=no_correct/len(res)\n",
        "print(\"테스트 집합에 대한 정확률은 \", accuracy*100, \"%입니다.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WooRaQqGa3cu",
        "outputId": "6d84da68-6335-4020-9eab-c7bb3422f8fd"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 1.88722261\n",
            "Iteration 2, loss = 0.32168255\n",
            "Iteration 3, loss = 0.22515980\n",
            "Iteration 4, loss = 0.17077716\n",
            "Iteration 5, loss = 0.13990340\n",
            "Iteration 6, loss = 0.11496338\n",
            "Iteration 7, loss = 0.10379415\n",
            "Iteration 8, loss = 0.08701073\n",
            "Iteration 9, loss = 0.07805352\n",
            "Iteration 10, loss = 0.07134141\n",
            "Iteration 11, loss = 0.06002547\n",
            "Iteration 12, loss = 0.05460012\n",
            "Iteration 13, loss = 0.05153631\n",
            "Iteration 14, loss = 0.04490045\n",
            "Iteration 15, loss = 0.04355680\n",
            "Iteration 16, loss = 0.04065333\n",
            "Iteration 17, loss = 0.03658824\n",
            "Iteration 18, loss = 0.03514514\n",
            "Iteration 19, loss = 0.03232551\n",
            "Iteration 20, loss = 0.03218663\n",
            "Iteration 21, loss = 0.03080589\n",
            "Iteration 22, loss = 0.02832055\n",
            "Iteration 23, loss = 0.02616064\n",
            "Iteration 24, loss = 0.02551886\n",
            "Iteration 25, loss = 0.02360337\n",
            "Iteration 26, loss = 0.02327202\n",
            "Iteration 27, loss = 0.02282711\n",
            "Iteration 28, loss = 0.02110618\n",
            "Iteration 29, loss = 0.01999608\n",
            "Iteration 30, loss = 0.01984492\n",
            "Iteration 31, loss = 0.01859590\n",
            "Iteration 32, loss = 0.01779473\n",
            "Iteration 33, loss = 0.01750404\n",
            "Iteration 34, loss = 0.01729497\n",
            "Iteration 35, loss = 0.01622759\n",
            "Iteration 36, loss = 0.01603377\n",
            "Iteration 37, loss = 0.01531042\n",
            "Iteration 38, loss = 0.01516596\n",
            "Iteration 39, loss = 0.01442970\n",
            "Iteration 40, loss = 0.01458956\n",
            "Iteration 41, loss = 0.01397956\n",
            "Iteration 42, loss = 0.01336566\n",
            "Iteration 43, loss = 0.01318289\n",
            "Iteration 44, loss = 0.01286906\n",
            "Iteration 45, loss = 0.01261276\n",
            "Iteration 46, loss = 0.01208310\n",
            "Iteration 47, loss = 0.01232009\n",
            "Iteration 48, loss = 0.01153540\n",
            "Iteration 49, loss = 0.01131032\n",
            "Iteration 50, loss = 0.01098174\n",
            "Iteration 51, loss = 0.01074734\n",
            "Iteration 52, loss = 0.01101253\n",
            "Iteration 53, loss = 0.01023124\n",
            "Iteration 54, loss = 0.01041367\n",
            "Iteration 55, loss = 0.01005311\n",
            "Iteration 56, loss = 0.00969679\n",
            "Iteration 57, loss = 0.00988005\n",
            "Iteration 58, loss = 0.00968247\n",
            "Iteration 59, loss = 0.00935277\n",
            "Iteration 60, loss = 0.00911187\n",
            "Iteration 61, loss = 0.00894046\n",
            "Iteration 62, loss = 0.00873887\n",
            "Iteration 63, loss = 0.00861532\n",
            "Iteration 64, loss = 0.00840243\n",
            "Iteration 65, loss = 0.00830276\n",
            "Iteration 66, loss = 0.00839076\n",
            "Iteration 67, loss = 0.00811067\n",
            "Iteration 68, loss = 0.00805181\n",
            "Iteration 69, loss = 0.00777565\n",
            "Iteration 70, loss = 0.00769901\n",
            "Iteration 71, loss = 0.00759748\n",
            "Iteration 72, loss = 0.00746333\n",
            "Iteration 73, loss = 0.00732781\n",
            "Iteration 74, loss = 0.00723759\n",
            "Iteration 75, loss = 0.00723287\n",
            "Iteration 76, loss = 0.00694883\n",
            "Iteration 77, loss = 0.00701659\n",
            "Iteration 78, loss = 0.00680150\n",
            "Iteration 79, loss = 0.00670452\n",
            "Iteration 80, loss = 0.00668000\n",
            "Iteration 81, loss = 0.00656863\n",
            "Iteration 82, loss = 0.00650762\n",
            "Iteration 83, loss = 0.00641426\n",
            "Iteration 84, loss = 0.00631719\n",
            "Iteration 85, loss = 0.00637467\n",
            "Iteration 86, loss = 0.00613781\n",
            "Iteration 87, loss = 0.00616510\n",
            "Iteration 88, loss = 0.00606791\n",
            "Iteration 89, loss = 0.00590897\n",
            "Iteration 90, loss = 0.00588789\n",
            "Iteration 91, loss = 0.00581658\n",
            "Iteration 92, loss = 0.00567523\n",
            "Iteration 93, loss = 0.00565205\n",
            "Iteration 94, loss = 0.00559060\n",
            "Iteration 95, loss = 0.00548678\n",
            "Iteration 96, loss = 0.00537940\n",
            "Iteration 97, loss = 0.00536506\n",
            "Iteration 98, loss = 0.00530831\n",
            "Iteration 99, loss = 0.00529588\n",
            "Iteration 100, loss = 0.00515538\n",
            "Iteration 101, loss = 0.00513683\n",
            "Iteration 102, loss = 0.00511289\n",
            "Iteration 103, loss = 0.00504107\n",
            "Iteration 104, loss = 0.00500116\n",
            "Iteration 105, loss = 0.00496172\n",
            "Iteration 106, loss = 0.00487402\n",
            "Iteration 107, loss = 0.00490146\n",
            "Iteration 108, loss = 0.00479959\n",
            "Iteration 109, loss = 0.00479398\n",
            "Iteration 110, loss = 0.00468227\n",
            "Iteration 111, loss = 0.00463128\n",
            "Iteration 112, loss = 0.00460426\n",
            "Iteration 113, loss = 0.00455103\n",
            "Iteration 114, loss = 0.00451107\n",
            "Iteration 115, loss = 0.00447344\n",
            "Iteration 116, loss = 0.00441550\n",
            "Iteration 117, loss = 0.00442485\n",
            "Iteration 118, loss = 0.00433989\n",
            "Iteration 119, loss = 0.00431869\n",
            "Iteration 120, loss = 0.00430408\n",
            "Iteration 121, loss = 0.00422180\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "[[65.  0.  0.  0.  0.  0.  3.  0.  0.  0.]\n",
            " [ 0. 64.  1.  0.  0.  0.  1.  0.  1.  0.]\n",
            " [ 0.  1. 69.  0.  0.  0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0. 83.  0.  0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0. 72.  0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0. 63.  0.  0.  0.  1.]\n",
            " [ 0.  0.  0.  0.  0.  0. 83.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.  0.  0. 69.  1.  0.]\n",
            " [ 0.  0.  0.  0.  0.  0.  0.  1. 59.  0.]\n",
            " [ 0.  0.  0.  1.  0.  1.  0.  1.  0. 79.]]\n",
            "테스트 집합에 대한 정확률은  98.19193324061196 %입니다.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "실행결과\n",
        "- 정확률이 SVM보다 약간 열등하고 퍼셉트론보다 우수한 것을 확인 가능\n",
        "\n",
        "MNIST 데이터셋으로 필기 숫자 데이터셋을\n",
        "다중 퍼셉트론으로 학습"
      ],
      "metadata": {
        "id": "XrJeg5csENFm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import fetch_openml\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "#MNIST 데이터셋을 읽고 훈련 집합과 테스트 집합으로 분할\n",
        "mnist=fetch_openml('mnist_784')\n",
        "mnist.data=mnist.data/255.0\n",
        "x_train=mnist.data[:60000]; x_test=mnist.data[60000:]\n",
        "y_train=np.int16(mnist.target[:60000]); y_test=np.int16(mnist.target[60000:])\n",
        "\n",
        "#MLP 분류기 모델을 학습\n",
        "mlp=MLPClassifier(hidden_layer_sizes=(100), learning_rate_init=0.001, batch_size=512, max_iter=300, solver='adam', verbose=True)\n",
        "mlp.fit(x_train,y_train)\n",
        "\n",
        "#테스트 집합으로 예측\n",
        "res=mlp.predict(x_test)\n",
        "\n",
        "#혼동 행렬\n",
        "conf=np.zeros((10,10),dtype=np.int16)\n",
        "for i in range(len(res)):\n",
        "  conf[res[i]][y_test[i]]+=1\n",
        "print(conf)\n",
        "\n",
        "#정확률 계산\n",
        "no_correct=0\n",
        "for i in range(10):\n",
        "  no_correct+=conf[i][i]\n",
        "accuracy=no_correct/len(res)\n",
        "print(\"테스트 집합에 대한 정확률은\", accuracy*100, \"%입니다.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nXAuwzhXEW9n",
        "outputId": "18890a7a-9ba8-48e5-9bc6-4622dbbc0d26"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 0.60144503\n",
            "Iteration 2, loss = 0.25678574\n",
            "Iteration 3, loss = 0.20236959\n",
            "Iteration 4, loss = 0.17001822\n",
            "Iteration 5, loss = 0.14623603\n",
            "Iteration 6, loss = 0.12839214\n",
            "Iteration 7, loss = 0.11446056\n",
            "Iteration 8, loss = 0.10269023\n",
            "Iteration 9, loss = 0.09284866\n",
            "Iteration 10, loss = 0.08497570\n",
            "Iteration 11, loss = 0.07786103\n",
            "Iteration 12, loss = 0.07105542\n",
            "Iteration 13, loss = 0.06561940\n",
            "Iteration 14, loss = 0.06047195\n",
            "Iteration 15, loss = 0.05591091\n",
            "Iteration 16, loss = 0.05147379\n",
            "Iteration 17, loss = 0.04823711\n",
            "Iteration 18, loss = 0.04450350\n",
            "Iteration 19, loss = 0.04138081\n",
            "Iteration 20, loss = 0.03860772\n",
            "Iteration 21, loss = 0.03555504\n",
            "Iteration 22, loss = 0.03331164\n",
            "Iteration 23, loss = 0.03113687\n",
            "Iteration 24, loss = 0.02892762\n",
            "Iteration 25, loss = 0.02730144\n",
            "Iteration 26, loss = 0.02514210\n",
            "Iteration 27, loss = 0.02372297\n",
            "Iteration 28, loss = 0.02196454\n",
            "Iteration 29, loss = 0.01995595\n",
            "Iteration 30, loss = 0.01888809\n",
            "Iteration 31, loss = 0.01784605\n",
            "Iteration 32, loss = 0.01634529\n",
            "Iteration 33, loss = 0.01563578\n",
            "Iteration 34, loss = 0.01450151\n",
            "Iteration 35, loss = 0.01355032\n",
            "Iteration 36, loss = 0.01222755\n",
            "Iteration 37, loss = 0.01163069\n",
            "Iteration 38, loss = 0.01099279\n",
            "Iteration 39, loss = 0.01034146\n",
            "Iteration 40, loss = 0.00942821\n",
            "Iteration 41, loss = 0.00912138\n",
            "Iteration 42, loss = 0.00836678\n",
            "Iteration 43, loss = 0.00772219\n",
            "Iteration 44, loss = 0.00716773\n",
            "Iteration 45, loss = 0.00659444\n",
            "Iteration 46, loss = 0.00619229\n",
            "Iteration 47, loss = 0.00564735\n",
            "Iteration 48, loss = 0.00548694\n",
            "Iteration 49, loss = 0.00504258\n",
            "Iteration 50, loss = 0.00470909\n",
            "Iteration 51, loss = 0.00429623\n",
            "Iteration 52, loss = 0.00402031\n",
            "Iteration 53, loss = 0.00395644\n",
            "Iteration 54, loss = 0.00353446\n",
            "Iteration 55, loss = 0.00335932\n",
            "Iteration 56, loss = 0.00319310\n",
            "Iteration 57, loss = 0.00296849\n",
            "Iteration 58, loss = 0.00284297\n",
            "Iteration 59, loss = 0.00270710\n",
            "Iteration 60, loss = 0.00265227\n",
            "Iteration 61, loss = 0.00234236\n",
            "Iteration 62, loss = 0.00221445\n",
            "Iteration 63, loss = 0.00200721\n",
            "Iteration 64, loss = 0.00197404\n",
            "Iteration 65, loss = 0.00177767\n",
            "Iteration 66, loss = 0.00171234\n",
            "Iteration 67, loss = 0.00163228\n",
            "Iteration 68, loss = 0.00170783\n",
            "Iteration 69, loss = 0.00146898\n",
            "Iteration 70, loss = 0.00141454\n",
            "Iteration 71, loss = 0.00135169\n",
            "Iteration 72, loss = 0.00130450\n",
            "Iteration 73, loss = 0.00116504\n",
            "Iteration 74, loss = 0.00112824\n",
            "Iteration 75, loss = 0.00106723\n",
            "Iteration 76, loss = 0.00100095\n",
            "Iteration 77, loss = 0.00104265\n",
            "Iteration 78, loss = 0.00112917\n",
            "Iteration 79, loss = 0.01145947\n",
            "Iteration 80, loss = 0.00380443\n",
            "Iteration 81, loss = 0.00134623\n",
            "Iteration 82, loss = 0.00093288\n",
            "Iteration 83, loss = 0.00085395\n",
            "Iteration 84, loss = 0.00080632\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "[[ 972    0    3    1    1    4    5    0    4    0]\n",
            " [   0 1123    4    0    0    1    4    5    0    3]\n",
            " [   1    3 1004    2    4    0    1    8    4    0]\n",
            " [   2    2    3  993    1   11    1    3    5    4]\n",
            " [   1    0    2    1  958    1    2    4    4   10]\n",
            " [   0    1    0    2    0  864    5    0    3    5]\n",
            " [   0    2    2    0    4    2  939    0    1    0]\n",
            " [   2    1    8    3    2    0    1 1001    4    6]\n",
            " [   2    3    5    5    2    6    0    2  945    3]\n",
            " [   0    0    1    3   10    3    0    5    4  978]]\n",
            "테스트 집합에 대한 정확률은 97.77 %입니다.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 하이퍼 매개변수 최적화"
      ],
      "metadata": {
        "id": "6A6Qf1NCGK3T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import datasets\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.model_selection  import train_test_split, validation_curve\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "\n",
        "# 데이터셋을 읽고 훈련 집합과 테스트 집합으로 분할\n",
        "digit=datasets.load_digits()\n",
        "x_train,x_test,y_train,y_test=train_test_split(digit.data,digit.target,train_size=0.6)\n",
        "\n",
        "#다층 퍼셉트론을 교차 검증으로 성능 평가 (소요시간 측정 포함)\n",
        "start=time.time()\n",
        "mlp=MLPClassifier(learning_rate_init=0.001, batch_size=32,max_iter=300, solver='sgd')\n",
        "prange=range(50,1001,50)\n",
        "train_score,test_score=validation_curve(mlp,x_train,y_train,param_name=\"hidden_layer_sizes\",param_range=prange,cv=10,scoring=\"accuracy\",n_jobs=4)\n",
        "end=time.time()\n",
        "print(\"하이퍼 매개변수 최적화에 걸린 시간은\",end-start,\"초입니다.\")\n",
        "\n",
        "#교차 검증 결과의 평균과 분산 구하기\n",
        "train_mean = np.mean(train_score,axis=1)\n",
        "train_std = np.std(train_score,axis=1)\n",
        "test_mean = np.mean(test_score,axis=1)\n",
        "test_std = np.std(test_score, axis=1)\n",
        "\n",
        "#성능 그래프 그리기\n",
        "plt.plot(prange,train_mean,label=\"Train score\", color=\"r\")\n",
        "plt.plot(prange, test_mean, label=\"Test score\", color=\"b\")\n",
        "plt.fill_between(prange, train_mean-train_std, train_mean+train_std, color=\"r\", alpha=0.2)\n",
        "plt.fill_between(prange, test_mean-test_std, test_mean+test_std, color=\"b\", alpha=0.2)\n",
        "plt.legend(loc=\"best\")\n",
        "plt.title(\"Validation Curve Mith MLP\")\n",
        "plt.xlabel(\"Number of hidden nodes\"); plt.ylabel(\"Accuracy\")\n",
        "plt.ylim(0.9,1.01)\n",
        "plt.grid(axis='both')\n",
        "plt.show()\n",
        "\n",
        "best_number_nodes=prange[np.argmax(test_mean)] # 최적의 은닉 노드 개수\n",
        "\n",
        "print(\"\\n최적의 은닉층의 노드 개수는\", best_number_nodes, \"개입니다.\\n\")\n",
        "\n",
        "# 최적의 은닉 노드 개수로 모델링\n",
        "mlp_test = MLPClassifier(hidden_layer_sizes=(best_number_nodes), learning_rate_init=0.001, batch_size=32, max_iter=300, solver='sgd')\n",
        "mlp_test.fit(x_train, y_train)\n",
        "\n",
        "# 테스트 집합으로 예측\n",
        "res = mlp_test.predict(x_test)\n",
        "\n",
        "# 혼동 행렬\n",
        "conf = np.zeros((10, 10))\n",
        "for i in range(len(res)):\n",
        "    conf[res[i]][y_test[i]] += 1\n",
        "print(conf)\n",
        "\n",
        "# 정확률 계산\n",
        "no_correct = 0\n",
        "for i in range(10):\n",
        "    no_correct += conf[i][i]\n",
        "accuracy = no_correct / len(res)\n",
        "print(\"테스트 집합에 대한 정확률은 \", accuracy * 100, \"%입니다.\")\n"
      ],
      "metadata": {
        "id": "hysfsm8SGQ8C"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}